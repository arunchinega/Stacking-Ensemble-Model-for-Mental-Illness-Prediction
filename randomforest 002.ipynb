{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f636269-a49d-409e-9dea-2364301d6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loaded successfully!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Mental Illness'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     26\u001b[0m columns_to_use \u001b[38;5;241m=\u001b[39m [  \u001b[38;5;66;03m# your full list of features including target\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# [same list as before, including 'Mental Illness']\u001b[39;00m\n\u001b[0;32m     28\u001b[0m ]\n\u001b[0;32m     30\u001b[0m data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[target_column], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m X \u001b[38;5;241m=\u001b[39m data[columns_to_use]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[target_column])\n\u001b[0;32m     32\u001b[0m y \u001b[38;5;241m=\u001b[39m data[target_column]\n\u001b[0;32m     33\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X, drop_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Mental Illness'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# --- 1. Import Libraries ---\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\" Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: File not found at {file_path}\")\n",
    "    raise\n",
    "# --- 3. Define Columns ---\n",
    "target_column = 'Mental Illness'\n",
    "columns_to_use = [  # your full list of features including target\n",
    "    # [same list as before, including 'Mental Illness']\n",
    "]\n",
    "\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "X = data[columns_to_use].drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# --- 4. Drop Multicollinear Columns ---\n",
    "columns_to_drop = [\n",
    "    # [same list of '_UNKNOWN' columns as before]\n",
    "]\n",
    "X_encoded.drop(columns=[col for col in columns_to_drop if col in X_encoded.columns], inplace=True)\n",
    "\n",
    "print(f\" Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# --- 5. Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\" Original class distribution:\", Counter(y_train))\n",
    "\n",
    "# --- 6. Apply SMOTE ---\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\" Resampled class distribution:\", Counter(y_train_res))\n",
    "\n",
    "# --- 7. Train Model ---\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\" Model training complete!\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"random_forest_model.pkl\")\n",
    "\n",
    "# --- 8. Evaluate Model ---\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\" Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 9. Feature Importance ---\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "print(\" Top 10 Features:\\n\", feature_importances.head(10))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importances.head(10).sort_values().plot(kind='barh')\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# --- 10. Additional Visualizations ---\n",
    "visual_df = data.copy()\n",
    "visual_df['Alcohol Related Disorder_UNKNOWN'] = visual_df['Alcohol Related Disorder'].apply(lambda x: 1 if x == 'UNKNOWN' else 0)\n",
    "visual_df['Intellectual Disability_UNKNOWN'] = visual_df['Intellectual Disability'].apply(lambda x: 1 if x == 'UNKNOWN' else 0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.countplot(data=visual_df, x='Alcohol Related Disorder_UNKNOWN', hue=target_column, ax=axes[0])\n",
    "axes[0].set_title('Mental Illness by Alcohol Related Disorder Status')\n",
    "axes[0].set_xlabel('Alcohol Related Disorder Status (0=Known, 1=Unknown)')\n",
    "\n",
    "sns.countplot(data=visual_df, x='Intellectual Disability_UNKNOWN', hue=target_column, ax=axes[1])\n",
    "axes[1].set_title('Mental Illness by Intellectual Disability Status')\n",
    "axes[1].set_xlabel('Intellectual Disability Status (0=Known, 1=Unknown)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"new_feature_visualizations.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2c729-7e8a-40b3-b306-2dd96d794ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Import Libraries ---\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: File not found at {file_path}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. Define Columns ---\n",
    "target_column = 'Mental Illness'\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness',\n",
    "    'Intellectual Disability', 'Autism Spectrum',\n",
    "    'Other Developmental Disability', 'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder', 'Opioid Related Disorder',\n",
    "    'Mobility Impairment Disorder', 'Hearing Impairment',\n",
    "    'Visual Impairment', 'Speech Impairment', 'Hyperlipidemia',\n",
    "    'High Blood Pressure', 'Diabetes', 'Obesity', 'Heart Attack',\n",
    "    'Stroke', 'Other Cardiac', 'Pulmonary Asthma',\n",
    "    'Alzheimer or Dementia', 'Kidney Disease', 'Liver Disease',\n",
    "    'Endocrine Condition', 'Neurological Condition',\n",
    "    'Traumatic Brain Injury', 'Joint Disease', 'Cancer',\n",
    "    'Other Chronic Med Condition', 'No Chronic Med Condition',\n",
    "    'Unknown Chronic Med Condition', 'Cannabis Recreational Use',\n",
    "    'Cannabis Medicinal Use', 'Smokes', 'Received Smoking Counseling',\n",
    "    'Serious Mental Illness', 'Alcohol 12m Service',\n",
    "    'Opioid 12m Service', 'Drug/Substance 12m Service',\n",
    "    'Criminal Justice Status'\n",
    "]\n",
    "\n",
    "# --- 4. Prepare Data ---\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "X = data[columns_to_use].drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# --- 5. Drop Multicollinear Columns ---\n",
    "columns_to_drop = [\n",
    "    'Household Composition_NOT APPLICABLE', 'Autism Spectrum_UNKNOWN', 'Other Developmental Disability_UNKNOWN',\n",
    "    'Drug Substance Disorder_UNKNOWN', 'Opioid Related Disorder_UNKNOWN', 'Mobility Impairment Disorder_UNKNOWN',\n",
    "    'Hearing Impairment_UNKNOWN', 'Visual Impairment_UNKNOWN', 'Speech Impairment_UNKNOWN',\n",
    "    'Hyperlipidemia_UNKNOWN', 'High Blood Pressure_UNKNOWN', 'Diabetes_UNKNOWN', 'Obesity_UNKNOWN',\n",
    "    'Heart Attack_UNKNOWN', 'Stroke_UNKNOWN', 'Other Cardiac_UNKNOWN', 'Pulmonary Asthma_UNKNOWN',\n",
    "    'Alzheimer or Dementia_UNKNOWN', 'Kidney Disease_UNKNOWN', 'Liver Disease_UNKNOWN',\n",
    "    'Endocrine Condition_UNKNOWN', 'Neurological Condition_UNKNOWN', 'Traumatic Brain Injury_UNKNOWN',\n",
    "    'Joint Disease_UNKNOWN', 'Cancer_UNKNOWN', 'Other Chronic Med Condition_UNKNOWN',\n",
    "    'No Chronic Med Condition_UNKNOWN', 'Alcohol 12m Service_UNKNOWN', 'Opioid 12m Service_UNKNOWN',\n",
    "    'Drug/Substance 12m Service_UNKNOWN'\n",
    "]\n",
    "X_encoded.drop(columns=[col for col in columns_to_drop if col in X_encoded.columns], inplace=True)\n",
    "\n",
    "print(f\"‚úÖ Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# --- 6. Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"üìä Original class distribution:\", Counter(y_train))\n",
    "\n",
    "# --- 7. Apply SMOTE ---\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"üìä Resampled class distribution:\", Counter(y_train_res))\n",
    "\n",
    "# --- 8. Train Model ---\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"‚úÖ Model training complete!\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"random_forest_model.pkl\")\n",
    "\n",
    "# --- 9. Evaluate Model ---\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"üéØ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"üìã Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68456e-c77f-4bd9-9e1c-aefb08eff9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 2. Load Data\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    raise\n",
    "\n",
    "# 3. Define Columns\n",
    "target_column = 'Mental Illness'\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness',\n",
    "    'Intellectual Disability', 'Autism Spectrum',\n",
    "    'Other Developmental Disability', 'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder', 'Opioid Related Disorder',\n",
    "    'Mobility Impairment Disorder', 'Hearing Impairment',\n",
    "    'Visual Impairment', 'Speech Impairment', 'Hyperlipidemia',\n",
    "    'High Blood Pressure', 'Diabetes', 'Obesity', 'Heart Attack',\n",
    "    'Stroke', 'Other Cardiac', 'Pulmonary Asthma',\n",
    "    'Alzheimer or Dementia', 'Kidney Disease', 'Liver Disease',\n",
    "    'Endocrine Condition', 'Neurological Condition',\n",
    "    'Traumatic Brain Injury', 'Joint Disease', 'Cancer',\n",
    "    'Other Chronic Med Condition', 'No Chronic Med Condition',\n",
    "    'Unknown Chronic Med Condition', 'Cannabis Recreational Use',\n",
    "    'Cannabis Medicinal Use', 'Smokes', 'Received Smoking Counseling',\n",
    "    'Serious Mental Illness', 'Alcohol 12m Service',\n",
    "    'Opioid 12m Service', 'Drug/Substance 12m Service',\n",
    "    'Criminal Justice Status'\n",
    "]\n",
    "\n",
    "# 4. Prepare Data\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "X = data[columns_to_use].drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 5. Drop Multicollinear Columns\n",
    "columns_to_drop = [\n",
    "    'Household Composition_NOT APPLICABLE', 'Autism Spectrum_UNKNOWN', 'Other Developmental Disability_UNKNOWN',\n",
    "    'Drug Substance Disorder_UNKNOWN', 'Opioid Related Disorder_UNKNOWN', 'Mobility Impairment Disorder_UNKNOWN',\n",
    "    'Hearing Impairment_UNKNOWN', 'Visual Impairment_UNKNOWN', 'Speech Impairment_UNKNOWN',\n",
    "    'Hyperlipidemia_UNKNOWN', 'High Blood Pressure_UNKNOWN', 'Diabetes_UNKNOWN', 'Obesity_UNKNOWN',\n",
    "    'Heart Attack_UNKNOWN', 'Stroke_UNKNOWN', 'Other Cardiac_UNKNOWN', 'Pulmonary Asthma_UNKNOWN',\n",
    "    'Alzheimer or Dementia_UNKNOWN', 'Kidney Disease_UNKNOWN', 'Liver Disease_UNKNOWN',\n",
    "    'Endocrine Condition_UNKNOWN', 'Neurological Condition_UNKNOWN', 'Traumatic Brain Injury_UNKNOWN',\n",
    "    'Joint Disease_UNKNOWN', 'Cancer_UNKNOWN', 'Other Chronic Med Condition_UNKNOWN',\n",
    "    'No Chronic Med Condition_UNKNOWN', 'Alcohol 12m Service_UNKNOWN', 'Opioid 12m Service_UNKNOWN',\n",
    "    'Drug/Substance 12m Service_UNKNOWN'\n",
    "]\n",
    "X_encoded.drop(columns=[col for col in columns_to_drop if col in X_encoded.columns], inplace=True)\n",
    "\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# 6. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"Original class distribution:\", Counter(y_train))\n",
    "\n",
    "# 7. Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"Resampled class distribution:\", Counter(y_train_res))\n",
    "\n",
    "# 8. Train Model\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"random_forest_model.pkl\")\n",
    "\n",
    "# 9. Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# 10. Feature Importance\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "print(\"Top 10 Features:\\n\", feature_importances.head(10))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importances.head(10).sort_values().plot(kind='barh')\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# 11. Additional Visualizations\n",
    "visual_df = data.copy()\n",
    "visual_df['Alcohol Related Disorder_UNKNOWN'] = visual_df['Alcohol Related Disorder'].apply(lambda x: 1 if x == 'UNKNOWN' else 0)\n",
    "visual_df['Intellectual Disability_UNKNOWN'] = visual_df['Intellectual Disability'].apply(lambda x: 1 if x == 'UNKNOWN' else 0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.countplot(data=visual_df, x='Alcohol Related Disorder_UNKNOWN', hue=target_column, ax=axes[0])\n",
    "axes[0].set_title('Mental Illness by Alcohol Related Disorder Status')\n",
    "axes[0].set_xlabel('Alcohol Related Disorder Status (0=Known, 1=Unknown)')\n",
    "\n",
    "sns.countplot(data=visual_df, x='Intellectual Disability_UNKNOWN', hue=target_column, ax=axes[1])\n",
    "axes[1].set_title('Mental Illness by Intellectual Disability Status')\n",
    "axes[1].set_xlabel('Intellectual Disability Status (0=Known, 1=Unknown)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"new_feature_visualizations.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99db8f-3c0a-40e2-8efb-d9eb0ef663aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all features are numeric\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f6d6d-ea71-474f-a545-5fa159616ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69f803-7b18-4e40-970b-25fef59e0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013376d3-7611-43a4-bc35-e7db7cde5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.dtype)\n",
    "print(y_train.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc408a-d6e4-4e25-b833-579e49a1506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target labels to numeric\n",
    "y_train = y_train.map({'YES': 1, 'NO': 0}).dropna()\n",
    "X_train = X_train.loc[y_train.index]  # align X with filtered y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756f8d9-5aa2-4da0-9684-9e05ae16bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"Resampled class distribution:\", Counter(y_train_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5857bc9-f2f8-437d-bd83-e5d7b2dc9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_res, y_train_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe13607-bde3-4796-bb85-726fd2e82d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a26442-4cbb-4bff-9c6c-cd52724837dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test to numeric\n",
    "y_test = y_test.map({'YES': 1, 'NO': 0}).dropna()\n",
    "X_test = X_test.loc[y_test.index]  # align X_test with filtered y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4df37-d469-4db2-9abb-825edf16b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c519f-a22f-4475-b24a-04a8c966511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'  #  This is the key change\n",
    ")\n",
    "\n",
    "model.fit(X_train_res, y_train_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760f4ed-f5a8-4ca6-8789-8e4b0f7218a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight='balanced'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d0ecb-d33f-495f-b676-c9cc09ba717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fe88f-fc70-4241-a120-36c4604944fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec89012-06d3-4d17-8687-ea92f08a9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c96e81-5b0d-4e48-a9a2-c3eb24b2de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1bb5a-2085-40d7-af87-415fd835fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and display top 10 features\n",
    "top_features = feature_importances.sort_values(ascending=False).head(10)\n",
    "print(top_features)\n",
    "\n",
    "# Plot them\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features.sort_values().plot(kind='barh')\n",
    "plt.title(\"Top 10 Most Important Features\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637173a-4690-4f9a-9fee-d5f15461d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df = df[df['Mental Illness'].isin(['YES', 'NO'])]\n",
    "\n",
    "# Convert target to numeric\n",
    "df['Mental Illness'] = df['Mental Illness'].map({'YES': 1, 'NO': 0})\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df)\n",
    "\n",
    "# Drop multicollinear _UNKNOWN columns\n",
    "columns_to_drop = [col for col in df_encoded.columns if '_UNKNOWN' in col]\n",
    "df_encoded.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop('Mental Illness', axis=1)\n",
    "y = df_encoded['Mental Illness']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train BalancedRandomForestClassifier\n",
    "brf = BalancedRandomForestClassifier(n_estimators=200, random_state=42)\n",
    "brf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = brf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c406f99-91af-42ce-bd6e-ed5e282004e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Suppress potential warnings from scikit-learn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Define File Path and Columns ---\n",
    "\n",
    "# The file path to your dataset.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "# The columns to be used in the model. This list has been refined based on our analysis.\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness',\n",
    "    'Intellectual Disability', 'Autism Spectrum',\n",
    "    'Other Developmental Disability', 'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder', 'Opioid Related Disorder',\n",
    "    'Mobility Impairment Disorder', 'Hearing Impairment',\n",
    "    'Visual Impairment', 'Speech Impairment', 'Hyperlipidemia',\n",
    "    'High Blood Pressure', 'Diabetes', 'Obesity', 'Heart Attack',\n",
    "    'Stroke', 'Other Cardiac', 'Pulmonary Asthma',\n",
    "    'Alzheimer or Dementia', 'Kidney Disease', 'Liver Disease',\n",
    "    'Endocrine Condition', 'Neurological Condition',\n",
    "    'Traumatic Brain Injury', 'Joint Disease', 'Cancer',\n",
    "    'Other Chronic Med Condition', 'No Chronic Med Condition',\n",
    "    'Unknown Chronic Med Condition', 'Cannabis Recreational Use',\n",
    "    'Cannabis Medicinal Use', 'Smokes', 'Received Smoking Counseling',\n",
    "    'Serious Mental Illness', 'Alcohol 12m Service',\n",
    "    'Opioid 12m Service', 'Drug/Substance 12m Service',\n",
    "    'Criminal Justice Status'\n",
    "]\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "target_column = 'Mental Illness'\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "X = data[columns_to_use].drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Remove multicollinear columns identified from our analysis\n",
    "columns_to_drop = [\n",
    "    'Household Composition_NOT APPLICABLE', 'Autism Spectrum_UNKNOWN', 'Other Developmental Disability_UNKNOWN',\n",
    "    'Drug Substance Disorder_UNKNOWN', 'Opioid Related Disorder_UNKNOWN', 'Mobility Impairment Disorder_UNKNOWN',\n",
    "    'Hearing Impairment_UNKNOWN', 'Visual Impairment_UNKNOWN', 'Speech Impairment_UNKNOWN',\n",
    "    'Hyperlipidemia_UNKNOWN', 'High Blood Pressure_UNKNOWN', 'Diabetes_UNKNOWN', 'Obesity_UNKNOWN',\n",
    "    'Heart Attack_UNKNOWN', 'Stroke_UNKNOWN', 'Other Cardiac_UNKNOWN', 'Pulmonary Asthma_UNKNOWN',\n",
    "    'Alzheimer or Dementia_UNKNOWN', 'Kidney Disease_UNKNOWN', 'Liver Disease_UNKNOWN',\n",
    "    'Endocrine Condition_UNKNOWN', 'Neurological Condition_UNKNOWN', 'Traumatic Brain Injury_UNKNOWN',\n",
    "    'Joint Disease_UNKNOWN', 'Cancer_UNKNOWN', 'Other Chronic Med Condition_UNKNOWN',\n",
    "    'No Chronic Med Condition_UNKNOWN', 'Alcohol 12m Service_UNKNOWN', 'Opioid 12m Service_UNKNOWN',\n",
    "    'Drug/Substance 12m Service_UNKNOWN'\n",
    "]\n",
    "\n",
    "for col in columns_to_drop:\n",
    "    if col in X_encoded.columns:\n",
    "        X_encoded = X_encoded.drop(columns=[col])\n",
    "\n",
    "print(\"\\nData has been cleaned and prepared for modeling.\")\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# Split the data into a training set (70%) and a testing set (30%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nOriginal class distribution in training data:\", Counter(y_train))\n",
    "\n",
    "# --- 3. Run a baseline model to get feature importances ---\n",
    "print(\"\\nRunning a baseline model to identify top features...\")\n",
    "baseline_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "feature_importances = pd.Series(baseline_model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "\n",
    "# Select top 20 features to reduce the data size for SMOTE\n",
    "top_features = feature_importances.head(20).index.tolist()\n",
    "print(f\"Selected top {len(top_features)} features for the final model.\")\n",
    "print(\"Top features are:\", top_features)\n",
    "\n",
    "# Filter the data to include only the top features\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "# --- 4. Address Class Imbalance with SMOTE on a smaller dataset ---\n",
    "\n",
    "print(\"\\nApplying SMOTE to the training data with top features to balance the classes...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_top, y_train)\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n",
    "\n",
    "# --- 5. Train the Random Forest Model on Balanced Data ---\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "print(\"\\nTraining the Random Forest model on the new, balanced data...\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# --- 6. Evaluate the Model ---\n",
    "\n",
    "y_pred = model.predict(X_test_top)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get the feature importances from the final model\n",
    "final_feature_importances = pd.Series(model.feature_importances_, index=X_train_top.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 most important features (Cleaned Model with SMOTE):\")\n",
    "print(final_feature_importances.head(10))\n",
    "\n",
    "# --- 7. Plot and Save Feature Importances ---\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "final_feature_importances.head(10).sort_values().plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Features for Predicting Mental Illness (Cleaned Model with SMOTE)')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "output_file = \"feature_importance_plot_cleaned_model_smote.png\"\n",
    "plt.savefig(output_file)\n",
    "print(f\"\\nPlot saved successfully as '{output_file}'\")\n",
    "plt.close()\n",
    "\n",
    "# --- 8. Visualize New Key Features ---\n",
    "\n",
    "print(\"\\nGenerating visualizations for new key features...\")\n",
    "visual_df = data.copy()\n",
    "# Create one-hot encoded columns for visualization\n",
    "visual_df['Alcohol Related Disorder_UNKNOWN'] = visual_df['Alcohol Related Disorder'].apply(lambda x: 1 if x == 'UNKNOWN' else 0)\n",
    "visual_df['Intellectual Disability_UNKNOWN'] = visual_df['Intellectual Disability'].apply(lambda x: 1 if x == 'UNKNOWN' else 0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.countplot(data=visual_df, x='Alcohol Related Disorder_UNKNOWN', hue=target_column, ax=axes[0])\n",
    "axes[0].set_title('Mental Illness by Alcohol Related Disorder Status')\n",
    "axes[0].set_xlabel('Alcohol Related Disorder Status (0=Known, 1=Unknown)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "sns.countplot(data=visual_df, x='Intellectual Disability_UNKNOWN', hue=target_column, ax=axes[1])\n",
    "axes[1].set_title('Mental Illness by Intellectual Disability Status')\n",
    "axes[1].set_xlabel('Intellectual Disability Status (0=Known, 1=Unknown)')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_file_visuals = \"new_feature_visualizations.png\"\n",
    "plt.savefig(output_file_visuals)\n",
    "print(f\"Visualizations for new key features saved as '{output_file_visuals}'\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf77c9-bcf0-4fbc-91ff-65edafcb0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Address Class Imbalance with SMOTE on a smaller dataset ---\n",
    "\n",
    "print(\"\\nApplying SMOTE to the training data with top features to balance the classes...\")\n",
    "\n",
    "# Ensure all features are numeric (convert bool to int)\n",
    "X_train_top = X_train_top.apply(lambda col: col.astype(int) if col.dtype == 'bool' else col)\n",
    "\n",
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_top, y_train)\n",
    "\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b7a5a-3c4e-47a5-91a2-8ac575516a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Suppress potential warnings from scikit-learn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Define File Path and Columns ---\n",
    "\n",
    "# The new file path to your social data subset.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Social data Part 2 new .csv\"\n",
    "\n",
    "# The columns relevant to this dataset, as requested.\n",
    "# The `Mental Illness` column is our target.\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness'\n",
    "]\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "# We need to make sure the data contains only the columns we need for this analysis\n",
    "# and handle any missing values in the target variable.\n",
    "data = data[columns_to_use]\n",
    "target_column = 'Mental Illness'\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Convert categorical features to dummy/indicator variables\n",
    "# We are not filtering for multicollinearity at this stage, as requested.\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(\"\\nData has been cleaned and prepared for modeling.\")\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# Split the data into a training set (70%) and a testing set (30%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nOriginal class distribution in training data:\", Counter(y_train))\n",
    "\n",
    "# --- 3. Address Class Imbalance with SMOTE ---\n",
    "\n",
    "print(\"\\nApplying SMOTE to the training data to balance the classes...\")\n",
    "# We will oversample the minority classes to a smaller, more manageable size\n",
    "target_counts = Counter(y_train)\n",
    "# Set the sampling strategy to oversample minority classes to a more balanced number\n",
    "sampling_strategy = {\n",
    "    'NO': int(target_counts['NO'] * 1.5),\n",
    "    'UNKNOWN': int(target_counts['UNKNOWN'] * 1.5),\n",
    "    'YES': target_counts['YES']\n",
    "}\n",
    "\n",
    "sm = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n",
    "\n",
    "# --- 4. Train the Random Forest Model on Balanced Data ---\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "print(\"\\nTraining the Random Forest model on the new, balanced data...\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# --- 5. Evaluate the Model ---\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get the feature importances from the final model\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))\n",
    "\n",
    "# --- 6. Plot and Save Feature Importances ---\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importances.head(10).sort_values().plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Social Features for Predicting Mental Illness (with SMOTE)')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "output_file = \"social_feature_importance_plot.png\"\n",
    "plt.savefig(output_file)\n",
    "print(f\"\\nPlot saved successfully as '{output_file}'\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c86971-8139-46dc-8fed-baf44634c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Social data Part 2 new .csv\"\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness'\n",
    "]\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Clean and Prepare\n",
    "data = data[columns_to_use]\n",
    "data = data[data['Mental Illness'].isin(['YES', 'NO'])]  # Remove rare/unknown classes\n",
    "target_column = 'Mental Illness'\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "print(\"\\nData has been cleaned and prepared for modeling.\")\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# 3. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(\"\\nOriginal class distribution in training data:\", Counter(y_train))\n",
    "\n",
    "# 4. Apply SMOTE\n",
    "print(\"\\nApplying SMOTE to the training data to balance the classes...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n",
    "\n",
    "# 5. Train Model\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "print(\"\\nTraining the Random Forest model on the new, balanced data...\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# 6. Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Feature Importances\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd0bad-b487-42f8-b6c6-5a63cf3f3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert boolean columns to integers before SMOTE\n",
    "X_train = X_train.apply(lambda col: col.astype(int) if col.dtype == 'bool' else col)\n",
    "\n",
    "# Apply SMOTE\n",
    "print(\"\\nApplying SMOTE to the training data to balance the classes...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfac10-c200-4ef8-bbb5-350786686f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Social data Part 2 new .csv\"\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness'\n",
    "]\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Clean and Prepare\n",
    "data = data[columns_to_use]\n",
    "data.dropna(subset=['Mental Illness'], inplace=True)\n",
    "data = data[data['Mental Illness'].isin(['YES', 'NO'])]\n",
    "\n",
    "X = data.drop(columns=['Mental Illness'])\n",
    "y = data['Mental Illness']\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "X_encoded = X_encoded.apply(lambda col: col.astype(int) if col.dtype == 'bool' else col)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Original class distribution in training data:\", Counter(y_train))\n",
    "\n",
    "# 4. Apply SMOTE\n",
    "print(\"\\nApplying SMOTE to the training data to balance the classes...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n",
    "\n",
    "# 5. Train Model\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# 6. Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Feature Importances\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548e855-f042-47b4-8587-9c3e2494d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Suppress potential warnings from scikit-learn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Define File Path and Columns ---\n",
    "\n",
    "# The new file path to your social data subset.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Social data Part 2 new .csv\"\n",
    "\n",
    "# The columns relevant to this dataset, as requested.\n",
    "# The `Mental Illness` column is our target.\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness'\n",
    "]\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "# We need to make sure the data contains only the columns we need for this analysis\n",
    "# and handle any missing values in the target variable.\n",
    "data = data[columns_to_use]\n",
    "target_column = 'Mental Illness'\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Convert categorical features to dummy/indicator variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(\"\\nData has been cleaned and prepared for modeling.\")\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# --- 3. Check for Multicollinearity ---\n",
    "\n",
    "print(\"\\nChecking for multicollinearity...\")\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = X_encoded.corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', mask=mask)\n",
    "plt.title('Correlation Matrix of Social Features')\n",
    "plt.savefig(\"social_correlation_matrix.png\")\n",
    "print(\"\\nCorrelation matrix heatmap plot saved as 'social_correlation_matrix.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Identify highly correlated features (e.g., correlation > 0.8)\n",
    "threshold = 0.8\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "if to_drop:\n",
    "    print(f\"\\nFeatures with multicollinearity (correlation > {threshold}):\")\n",
    "    print(to_drop)\n",
    "else:\n",
    "    print(f\"\\nNo features with multicollinearity found above the {threshold} threshold.\")\n",
    "\n",
    "# The rest of the code for SMOTE, model training, and visualizations will go in the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfddbb-a529-4858-ad6d-affda3663bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Suppress potential warnings from scikit-learn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Define File Path and Columns ---\n",
    "\n",
    "# The new file path to your social data subset.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Social data Part 2 new .csv\"\n",
    "\n",
    "# The columns relevant to this dataset, as requested.\n",
    "# The `Mental Illness` column is our target.\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness'\n",
    "]\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "# We need to make sure the data contains only the columns we need for this analysis\n",
    "# and handle any missing values in the target variable.\n",
    "data = data[columns_to_use]\n",
    "target_column = 'Mental Illness'\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Convert categorical features to dummy/indicator variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(\"\\nData has been cleaned and prepared for modeling.\")\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# --- 3. Check for Multicollinearity ---\n",
    "\n",
    "print(\"\\nChecking for multicollinearity...\")\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = X_encoded.corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', mask=mask)\n",
    "plt.title('Correlation Matrix of Social Features')\n",
    "plt.savefig(\"social_correlation_matrix.png\")\n",
    "print(\"\\nCorrelation matrix heatmap plot saved as 'social_correlation_matrix.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Identify highly correlated features (e.g., correlation > 0.8)\n",
    "threshold = 0.8\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "if to_drop:\n",
    "    print(f\"\\nFeatures with multicollinearity (correlation > {threshold}):\")\n",
    "    print(to_drop)\n",
    "else:\n",
    "    print(f\"\\nNo features with multicollinearity found above the {threshold} threshold.\")\n",
    "\n",
    "# The rest of the code for SMOTE, model training, and visualizations will go in the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9b949-499f-45e9-a9f1-d6c3cd73e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Suppress potential warnings from scikit-learn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Define File Path and Columns ---\n",
    "\n",
    "# The new file path to your social data subset.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Social data Part 2 new .csv\"\n",
    "\n",
    "# The columns relevant to this dataset, as requested.\n",
    "# The `Mental Illness` column is our target.\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness'\n",
    "]\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "# We need to make sure the data contains only the columns we need for this analysis\n",
    "# and handle any missing values in the target variable.\n",
    "data = data[columns_to_use]\n",
    "target_column = 'Mental Illness'\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Convert categorical features to dummy/indicator variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(\"\\nData has been cleaned and prepared for modeling.\")\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# --- 3. Check for Multicollinearity ---\n",
    "\n",
    "print(\"\\nChecking for multicollinearity...\")\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = X_encoded.corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', mask=mask)\n",
    "plt.title('Correlation Matrix of Social Features')\n",
    "plt.savefig(\"social_correlation_matrix.png\")\n",
    "print(\"\\nCorrelation matrix heatmap plot saved as 'social_correlation_matrix.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Identify highly correlated features (e.g., correlation > 0.8)\n",
    "threshold = 0.8\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "if to_drop:\n",
    "    print(f\"\\nFeatures with multicollinearity (correlation > {threshold}):\")\n",
    "    print(to_drop)\n",
    "else:\n",
    "    print(f\"\\nNo features with multicollinearity found above the {threshold} threshold.\")\n",
    "\n",
    "# --- Drop Multicollinear Features ---\n",
    "if 'Household Composition_NOT APPLICABLE' in X_encoded.columns:\n",
    "    X_encoded = X_encoded.drop(columns=['Household Composition_NOT APPLICABLE'])\n",
    "    print(\"\\nRemoved multicollinear feature: Household Composition_NOT APPLICABLE\")\n",
    "\n",
    "# Split the data into a training set (70%) and a testing set (30%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nOriginal class distribution in training data:\", Counter(y_train))\n",
    "\n",
    "# --- 4. Address Class Imbalance with SMOTE ---\n",
    "\n",
    "print(\"\\nApplying SMOTE to the training data to balance the classes...\")\n",
    "# We will oversample the minority classes to a smaller, more manageable size\n",
    "target_counts = Counter(y_train)\n",
    "# Set the sampling strategy to oversample minority classes to a more balanced number\n",
    "sampling_strategy = {\n",
    "    'NO': int(target_counts['NO'] * 1.5),\n",
    "    'UNKNOWN': int(target_counts['UNKNOWN'] * 1.5),\n",
    "    'YES': target_counts['YES']\n",
    "}\n",
    "\n",
    "sm = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n",
    "\n",
    "# --- 5. Train the Random Forest Model on Balanced Data ---\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "print(\"\\nTraining the Random Forest model on the new, balanced data...\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# --- 6. Evaluate the Model ---\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get the feature importances from the final model\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))\n",
    "\n",
    "# --- 7. Plot and Save Feature Importances ---\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importances.head(10).sort_values().plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Social Features for Predicting Mental Illness (with SMOTE)')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "output_file = \"social_feature_importance_plot.png\"\n",
    "plt.savefig(output_file)\n",
    "print(f\"\\nPlot saved successfully as '{output_file}'\")\n",
    "plt.close()\n",
    "\n",
    "# --- 8. Generate new visualizations for key features ---\n",
    "print(\"\\nGenerating new visualizations for key features...\")\n",
    "\n",
    "# Create a figure with a subplot for each plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Relationship Between Top Social Features and Mental Illness', fontsize=18)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 1: Special Education Services vs. Mental Illness\n",
    "sns.countplot(data=data, x='Special Education Services', hue=target_column, ax=axes[0])\n",
    "axes[0].set_title('Mental Illness by Special Education Services')\n",
    "axes[0].set_xlabel('Special Education Services')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Plot 2: Household Composition vs. Mental Illness\n",
    "sns.countplot(data=data, x='Household Composition', hue=target_column, ax=axes[1])\n",
    "axes[1].set_title('Mental Illness by Household Composition')\n",
    "axes[1].set_xlabel('Household Composition')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Race vs. Mental Illness\n",
    "sns.countplot(data=data, x='Race', hue=target_column, ax=axes[2])\n",
    "axes[2].set_title('Mental Illness by Race')\n",
    "axes[2].set_xlabel('Race')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Religious Preference vs. Mental Illness\n",
    "sns.countplot(data=data, x='Religious Preference', hue=target_column, ax=axes[3])\n",
    "axes[3].set_title('Mental Illness by Religious Preference')\n",
    "axes[3].set_xlabel('Religious Preference')\n",
    "axes[3].set_ylabel('Count')\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "output_file_visuals = \"social_feature_visualizations.png\"\n",
    "plt.savefig(output_file_visuals)\n",
    "print(f\"New visualizations for key social features saved as '{output_file_visuals}'\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a160b68b-d5b6-41a4-9e39-17a417e5cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "\n",
      "Data has been cleaned and prepared for modeling.\n",
      "Final feature set shape: (99244, 48)\n",
      "\n",
      "Checking for multicollinearity...\n",
      "\n",
      "Correlation matrix heatmap plot saved as 'social_correlation_matrix.png'\n",
      "\n",
      "Features with multicollinearity (correlation > 0.8):\n",
      "['Household Composition_NOT APPLICABLE']\n",
      "\n",
      "Removed multicollinear feature: Household Composition_NOT APPLICABLE\n",
      "\n",
      "Original class distribution in training data: Counter({'YES': 66984, 'NO': 1794, 'UNKNOWN': 692})\n",
      "\n",
      "Applying SMOTE to the training data to balance the classes...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 104\u001b[0m\n\u001b[0;32m     97\u001b[0m sampling_strategy \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(target_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m),\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(target_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m),\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m'\u001b[39m: target_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    101\u001b[0m }\n\u001b[0;32m    103\u001b[0m sm \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39msampling_strategy, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m X_train_res, y_train_res \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew class distribution in training data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Counter(y_train_res))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# --- 5. Train the Random Forest Model on Balanced Data ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:105\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m     99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    107\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    109\u001b[0m )\n\u001b[0;32m    111\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:360\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[0;32m    359\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 360\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[0;32m    361\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    362\u001b[0m )\n\u001b[0;32m    363\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n\u001b[0;32m    364\u001b[0m y_resampled\u001b[38;5;241m.\u001b[39mappend(y_new)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:118\u001b[0m, in \u001b[0;36mBaseSMOTE._make_samples\u001b[1;34m(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size, y)\u001b[0m\n\u001b[0;32m    115\u001b[0m rows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor_divide(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    116\u001b[0m cols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmod(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 118\u001b[0m X_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_samples(X, nn_data, nn_num, rows, cols, steps, y_type, y)\n\u001b[0;32m    119\u001b[0m y_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(n_samples, fill_value\u001b[38;5;241m=\u001b[39my_type, dtype\u001b[38;5;241m=\u001b[39my_dtype)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_new, y_new\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:172\u001b[0m, in \u001b[0;36mBaseSMOTE._generate_samples\u001b[1;34m(self, X, nn_data, nn_num, rows, cols, steps, y_type, y)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_samples\u001b[39m(\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, nn_data, nn_num, rows, cols, steps, y_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m ):\n\u001b[0;32m    125\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a synthetic sample.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    The rule for the generation is:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m        Synthetically generated samples.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m     diffs \u001b[38;5;241m=\u001b[39m nn_data[nn_num[rows, cols]] \u001b[38;5;241m-\u001b[39m X[rows]\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# only entering for BorderlineSMOTE-2\u001b[39;00m\n\u001b[0;32m    174\u001b[0m         random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "\u001b[1;31mTypeError\u001b[0m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Suppress potential warnings from scikit-learn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Define File Path and Columns ---\n",
    "\n",
    "# The new file path to your social data subset.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Social data Part 2 new .csv\"\n",
    "\n",
    "# The columns relevant to this dataset, as requested.\n",
    "# The `Mental Illness` column is our target.\n",
    "columns_to_use = [\n",
    "    'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Mental Illness'\n",
    "]\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "# We need to make sure the data contains only the columns we need for this analysis\n",
    "# and handle any missing values in the target variable.\n",
    "data = data[columns_to_use]\n",
    "target_column = 'Mental Illness'\n",
    "data.dropna(subset=[target_column], inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Convert categorical features to dummy/indicator variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(\"\\nData has been cleaned and prepared for modeling.\")\n",
    "print(f\"Final feature set shape: {X_encoded.shape}\")\n",
    "\n",
    "# --- 3. Check for Multicollinearity ---\n",
    "\n",
    "print(\"\\nChecking for multicollinearity...\")\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = X_encoded.corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', mask=mask)\n",
    "plt.title('Correlation Matrix of Social Features')\n",
    "plt.savefig(\"social_correlation_matrix.png\")\n",
    "print(\"\\nCorrelation matrix heatmap plot saved as 'social_correlation_matrix.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Identify highly correlated features (e.g., correlation > 0.8)\n",
    "threshold = 0.8\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "if to_drop:\n",
    "    print(f\"\\nFeatures with multicollinearity (correlation > {threshold}):\")\n",
    "    print(to_drop)\n",
    "else:\n",
    "    print(f\"\\nNo features with multicollinearity found above the {threshold} threshold.\")\n",
    "\n",
    "# --- Drop Multicollinear Features ---\n",
    "if 'Household Composition_NOT APPLICABLE' in X_encoded.columns:\n",
    "    X_encoded = X_encoded.drop(columns=['Household Composition_NOT APPLICABLE'])\n",
    "    print(\"\\nRemoved multicollinear feature: Household Composition_NOT APPLICABLE\")\n",
    "\n",
    "# Split the data into a training set (70%) and a testing set (30%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nOriginal class distribution in training data:\", Counter(y_train))\n",
    "\n",
    "# --- 4. Address Class Imbalance with SMOTE ---\n",
    "\n",
    "print(\"\\nApplying SMOTE to the training data to balance the classes...\")\n",
    "# We will oversample the minority classes to a smaller, more manageable size\n",
    "target_counts = Counter(y_train)\n",
    "# Set the sampling strategy to oversample minority classes to a more balanced number\n",
    "sampling_strategy = {\n",
    "    'NO': int(target_counts['NO'] * 1.5),\n",
    "    'UNKNOWN': int(target_counts['UNKNOWN'] * 1.5),\n",
    "    'YES': target_counts['YES']\n",
    "}\n",
    "\n",
    "sm = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"New class distribution in training data:\", Counter(y_train_res))\n",
    "\n",
    "# --- 5. Train the Random Forest Model on Balanced Data ---\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "print(\"\\nTraining the Random Forest model on the new, balanced data...\")\n",
    "model.fit(X_train_res, y_train_res)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# --- 6. Evaluate the Model ---\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get the feature importances from the final model\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(feature_importances.head(10))\n",
    "\n",
    "# --- 7. Plot and Save Feature Importances ---\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importances.head(10).sort_values().plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Social Features for Predicting Mental Illness (with SMOTE)')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "output_file = \"social_feature_importance_plot.png\"\n",
    "plt.savefig(output_file)\n",
    "print(f\"\\nPlot saved successfully as '{output_file}'\")\n",
    "plt.close()\n",
    "\n",
    "# --- 8. Generate new visualizations for key features ---\n",
    "print(\"\\nGenerating new visualizations for key features...\")\n",
    "\n",
    "# Create a figure with a subplot for each plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Relationship Between Top Social Features and Mental Illness', fontsize=18)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 1: Special Education Services vs. Mental Illness\n",
    "sns.countplot(data=data, x='Special Education Services', hue=target_column, ax=axes[0])\n",
    "axes[0].set_title('Mental Illness by Special Education Services')\n",
    "axes[0].set_xlabel('Special Education Services')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Plot 2: Household Composition vs. Mental Illness\n",
    "sns.countplot(data=data, x='Household Composition', hue=target_column, ax=axes[1])\n",
    "axes[1].set_title('Mental Illness by Household Composition')\n",
    "axes[1].set_xlabel('Household Composition')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Race vs. Mental Illness\n",
    "sns.countplot(data=data, x='Race', hue=target_column, ax=axes[2])\n",
    "axes[2].set_title('Mental Illness by Race')\n",
    "axes[2].set_xlabel('Race')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Religious Preference vs. Mental Illness\n",
    "sns.countplot(data=data, x='Religious Preference', hue=target_column, ax=axes[3])\n",
    "axes[3].set_title('Mental Illness by Religious Preference')\n",
    "axes[3].set_xlabel('Religious Preference')\n",
    "axes[3].set_ylabel('Count')\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "output_file_visuals = \"social_feature_visualizations.png\"\n",
    "plt.savefig(output_file_visuals)\n",
    "print(f\"New visualizations for key social features saved as '{output_file_visuals}'\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c903d5-8670-4fb1-8f83-29514da600ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
