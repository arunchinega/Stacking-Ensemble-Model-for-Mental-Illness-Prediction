{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59661b2-8af8-40f7-b7f6-bf266993a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"Female Mental Illness Data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "file_path = \"Female Mental Illness Data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 3: Filter out rows where 'Mental Illness' is marked as 'UNKNOWN'\n",
    "df_filtered = df[df['Mental Illness'].str.upper() != 'UNKNOWN']\n",
    "\n",
    "# Step 4: Select relevant categorical columns for one-hot encoding\n",
    "categorical_columns = [\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status'\n",
    "]\n",
    "\n",
    "# Step 5: Apply one-hot encoding to the selected columns\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 6: Preview the result\n",
    "print(\"Encoded DataFrame shape:\", df_encoded.shape)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d38820-a964-438f-8b0d-7da027d4ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Load the dataset from the correct path\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 3: Filter out rows where 'Mental Illness' is marked as 'UNKNOWN'\n",
    "df_filtered = df[df['Mental Illness'].str.upper() != 'UNKNOWN']\n",
    "\n",
    "# Step 4: Select relevant categorical columns for one-hot encoding\n",
    "categorical_columns = [\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status'\n",
    "]\n",
    "\n",
    "# Step 5: Apply one-hot encoding to the selected columns\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Step 6: Preview the result\n",
    "print(\"Encoded DataFrame shape:\", df_encoded.shape)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d4acb-c808-4b75-8297-6d29810631c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity\n",
    "# Step 1: Run KMO Test\n",
    "kmo_all, kmo_model = calculate_kmo(df_encoded)\n",
    "print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "print(\"Interpretation:\", \"Acceptable\" if kmo_model >= 0.6 else \"Too low for Factor Analysis\")\n",
    "\n",
    "# Step 2: Run Bartlett’s Test\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(df_encoded)\n",
    "print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "print(\"Interpretation:\", \"Suitable for Factor Analysis\" if p_value < 0.05 else \"Not suitable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd172a-34cf-4498-874f-4deb19d55f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity\n",
    "\n",
    "# Step 1: Run KMO Test\n",
    "kmo_all, kmo_model = calculate_kmo(df_encoded)\n",
    "print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "print(\"Interpretation:\", \"Acceptable\" if kmo_model >= 0.6 else \"Too low for Factor Analysis\")\n",
    "\n",
    "# Step 2: Run Bartlett’s Test\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(df_encoded)\n",
    "print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "print(\"Interpretation:\", \"Suitable for Factor Analysis\" if p_value < 0.05 else \"Not suitable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476306a-0423-4b58-9bae-bf2916c9ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install factor-analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae09b4-e8cf-4b34-b499-03ac0f7d24a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install factor-analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a325f-3d17-40ca-877e-80094d27215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas factor_analyzer scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c4ba18-a3a5-49cd-a24d-eb88afa784b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Your file path\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(\"Success: File found!\")\n",
    "    df_check = pd.read_csv(file_path)\n",
    "    print(\"\\nColumns in your file:\")\n",
    "    print(df_check.columns.tolist())\n",
    "else:\n",
    "    print(\"Error: File not found. Please double-check the file path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21c204-308d-481f-aaa5-60a55fa8863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "import warnings\n",
    "\n",
    "# We are using a new, simplified approach as you requested.\n",
    "# This code will only perform Factor Analysis on the columns you provided.\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "# This list includes only the columns you specified for analysis.\n",
    "columns_for_analysis = [\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Select only the specified columns.\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    # The code will stop here if the column names are incorrect.\n",
    "    # If this happens, you must check the names in your file.\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "# As you confirmed, all values are non-numeric. This converts your categorical\n",
    "# data into a numeric format suitable for analysis.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "\n",
    "# 5. Run prerequisite tests (KMO and Bartlett's Test).\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "# We'll automatically determine the number of factors based on eigenvalues > 1.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73084816-68ef-483d-81ce-1c1f747625b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fdff8-186a-4358-9619-638e51c20cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python?code_reference&code_event_index=2\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "import warnings\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "# This list includes only the columns you specified for analysis.\n",
    "columns_for_analysis = [\n",
    "    'Sexual Orientation',\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Veteran Status',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Select only the specified columns.\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "\n",
    "# 5. Run prerequisite tests (KMO and Bartlett's Test).\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings in tabular form.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)\n",
    "```\n",
    "\n",
    "```text?code_stderr&code_event_index=2\n",
    "Traceback (most recent call last):\n",
    "  File \"<string>\", line 2, in <module>\n",
    "ModuleNotFoundError: No module named 'factor_analyzer'\n",
    "\n",
    "```\n",
    "\n",
    "You're right. My apologies, I missed a column in the previous code I generated. The list you provided is: `Sexual Orientation`, `Living Situation`, `Household Composition`, `Religious Preference`, `Veteran Status`, `Employment Status`, `Number Of Hours Worked Each Week`, `Education Status`, `Mental Illness`. Let's include all of these in the code.\n",
    "\n",
    "Here's the corrected code for you to run. I've included the installation command in the same block to make sure it's installed and available for this notebook session.\n",
    "\n",
    "```python\n",
    "# First, run the installation command in the same notebook session.\n",
    "!pip install factor_analyzer\n",
    "\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "import warnings\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "# This list includes all the columns you specified for analysis.\n",
    "columns_for_analysis = [\n",
    "    'Sexual Orientation',\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Veteran Status',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Select only the specified columns.\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "\n",
    "# 5. Run prerequisite tests (KMO and Bartlett's Test).\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214e846-55e4-41e9-bdff-a4b551a29c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "!pip install factor_analyzer\n",
    "\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "import warnings\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "# CORRECTED list of columns for analysis.\n",
    "columns_for_analysis = [\n",
    "    'Sexual Orientation',\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Veteran Status',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Select only the specified columns.\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "\n",
    "# 5. Run prerequisite tests (KMO and Bartlett's Test).\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee2674-3abb-41be-aeb5-7bddc730093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "!pip install factor_analyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "# Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Sexual Orientation',\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Veteran Status',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Select only the specified columns.\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Multicollinearity Check ---\n",
    "print(\"\\n--- Correlation Matrix ---\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_processed.corr()\n",
    "\n",
    "# Display the full correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Optionally, we can also highlight pairs with high correlation\n",
    "print(\"\\n--- Highly Correlated Variable Pairs (Absolute value > 0.75) ---\")\n",
    "\n",
    "# Create a mask to hide the lower triangle and diagonal\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "upper_triangle = correlation_matrix.mask(mask)\n",
    "\n",
    "# Find pairs with high correlation\n",
    "highly_correlated = upper_triangle.stack()[upper_triangle.stack().abs() > 0.75]\n",
    "print(highly_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbec16-b6f1-48aa-8664-f4cf7c719073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "import warnings\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "# CORRECTED list of columns for analysis.\n",
    "# Removed 'Household Composition' to fix multicollinearity.\n",
    "columns_for_analysis = [\n",
    "    'Sexual Orientation',\n",
    "    'Living Situation',\n",
    "    'Religious Preference',\n",
    "    'Veteran Status',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1073c7-1ff3-4d72-b3f2-2c0a62292e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Sexual Orientation',\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Veteran Status',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)\n",
    "\n",
    "# --- Part 3: Regression Analysis (Connecting Factors to Mental Illness) ---\n",
    "print(\"\\n--- Running Regression Analysis on the Factors ---\")\n",
    "\n",
    "# 10. Get the factor scores for each observation\n",
    "factor_scores = fa_final.transform(df_processed)\n",
    "factor_scores_df = pd.DataFrame(factor_scores, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "\n",
    "# 11. Prepare the data for regression\n",
    "target_variable = 'Mental Illness'\n",
    "regression_df = factor_scores_df.copy()\n",
    "regression_df[target_variable] = df[target_variable].map({'YES': 1, 'NO': 0, 'UNKNOWN': None})\n",
    "\n",
    "# Drop rows with unknown values for the target variable\n",
    "regression_df.dropna(subset=[target_variable], inplace=True)\n",
    "X = regression_df.drop(columns=[target_variable])\n",
    "y = regression_df[target_variable]\n",
    "\n",
    "# 12. Train and fit a Logistic Regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 13. Interpret the regression coefficients\n",
    "print(\"\\n--- Regression Coefficients of Factors Predicting Mental Illness ---\")\n",
    "regression_coefficients = pd.DataFrame(model.coef_.T, index=X.columns, columns=['Coefficient'])\n",
    "regression_coefficients['Absolute Value'] = regression_coefficients['Coefficient'].abs()\n",
    "regression_coefficients = regression_coefficients.sort_values(by='Absolute Value', ascending=False)\n",
    "\n",
    "print(\"Higher absolute coefficient values indicate a stronger impact on the probability of mental illness.\")\n",
    "print(regression_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2bcb6-7729-4618-872c-70cf31a22464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "!pip install factor_analyzer\n",
    "\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "\n",
    "# 5. Run prerequisite tests (KMO and Bartlett's Test).\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings and save to CSV.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)\n",
    "loadings_df.to_csv(\"factor_loadings.csv\")\n",
    "print(\"\\nFactor loadings saved to 'factor_loadings.csv'\")\n",
    "\n",
    "# --- Part 3: Regression Analysis (Connecting Factors to Mental Illness) ---\n",
    "print(\"\\n--- Running Regression Analysis on the Factors ---\")\n",
    "\n",
    "# 10. Get the factor scores for each observation\n",
    "factor_scores = fa_final.transform(df_processed)\n",
    "factor_scores_df = pd.DataFrame(factor_scores, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "\n",
    "# 11. Prepare the data for regression\n",
    "target_variable = 'Mental Illness'\n",
    "regression_df = factor_scores_df.copy()\n",
    "regression_df[target_variable] = df[target_variable].map({'YES': 1, 'NO': 0, 'UNKNOWN': None})\n",
    "\n",
    "# Drop rows with unknown values for the target variable\n",
    "regression_df.dropna(subset=[target_variable], inplace=True)\n",
    "X = regression_df.drop(columns=[target_variable])\n",
    "y = regression_df[target_variable]\n",
    "\n",
    "# 12. Train and fit a Logistic Regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 13. Interpret the regression coefficients and save to CSV.\n",
    "print(\"\\n--- Regression Coefficients of Factors Predicting Mental Illness ---\")\n",
    "regression_coefficients = pd.DataFrame(model.coef_.T, index=X.columns, columns=['Coefficient'])\n",
    "regression_coefficients['Absolute Value'] = regression_coefficients['Coefficient'].abs()\n",
    "regression_coefficients = regression_coefficients.sort_values(by='Absolute Value', ascending=False)\n",
    "print(regression_coefficients)\n",
    "\n",
    "# Save the regression coefficients to a CSV file\n",
    "regression_coefficients.to_csv(\"regression_coefficients.csv\")\n",
    "print(\"\\nRegression coefficients saved to 'regression_coefficients.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c33987-a442-4938-90f2-f440adf6675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "# We're running this again for safety since we're starting over.\n",
    "!pip install factor_analyzer\n",
    "\n",
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female Social data .csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Living Situation',\n",
    "    'Household Composition',\n",
    "    'Religious Preference',\n",
    "    'Employment Status',\n",
    "    'Number Of Hours Worked Each Week',\n",
    "    'Education Status',\n",
    "    'Mental Illness'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, drop_first=True)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# --- Part 2: Factor Analysis ---\n",
    "print(\"\\n--- Running Factor Analysis Prerequisites ---\")\n",
    "\n",
    "# 5. Run prerequisite tests (KMO and Bartlett's Test).\n",
    "if df_processed.shape[1] < 2:\n",
    "    print(\"Not enough variables for Factor Analysis.\")\n",
    "else:\n",
    "    kmo_all, kmo_model = calculate_kmo(df_processed)\n",
    "    print(f\"KMO Test Statistic: {kmo_model:.4f}\")\n",
    "    if kmo_model < 0.6:\n",
    "        print(\"Interpretation: KMO is too low. Factor Analysis may not be appropriate.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: KMO is acceptable.\\n\")\n",
    "\n",
    "    chi_square_value, p_value = calculate_bartlett_sphericity(df_processed)\n",
    "    print(f\"Bartlett's Test Chi-Square: {chi_square_value:.2f}\")\n",
    "    print(f\"Bartlett's Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Interpretation: The p-value is significant. Data is suitable.\\n\")\n",
    "    else:\n",
    "        print(\"Interpretation: The p-value is not significant. Data is not suitable.\\n\")\n",
    "\n",
    "# 6. Fit the Factor Analysis model.\n",
    "fa = FactorAnalyzer(n_factors=df_processed.shape[1], rotation=\"varimax\")\n",
    "fa.fit(df_processed)\n",
    "\n",
    "# 7. Get Eigenvalues and determine the number of factors to retain.\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "num_factors = sum(eigenvalues > 1)\n",
    "print(f\"\\nNumber of factors to retain (Eigenvalues > 1): {num_factors}\")\n",
    "\n",
    "# 8. Re-fit the model with the determined number of factors.\n",
    "fa_final = FactorAnalyzer(n_factors=num_factors, rotation=\"varimax\")\n",
    "fa_final.fit(df_processed)\n",
    "\n",
    "# 9. Display Factor Loadings and save to CSV.\n",
    "print(\"\\n--- Factor Loadings (Variable correlations with each factor) ---\")\n",
    "loadings_df = pd.DataFrame(fa_final.loadings_, index=df_processed.columns, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "print(loadings_df)\n",
    "loadings_df.to_csv(\"factor_loadings.csv\")\n",
    "print(\"\\nFactor loadings saved to 'factor_loadings.csv'\")\n",
    "\n",
    "# --- Part 3: Regression Analysis (Connecting Factors to Mental Illness) ---\n",
    "print(\"\\n--- Running Regression Analysis on the Factors ---\")\n",
    "\n",
    "# 10. Get the factor scores for each observation\n",
    "factor_scores = fa_final.transform(df_processed)\n",
    "factor_scores_df = pd.DataFrame(factor_scores, columns=[f'Factor {i+1}' for i in range(num_factors)])\n",
    "\n",
    "# 11. Prepare the data for regression\n",
    "target_variable = 'Mental Illness'\n",
    "regression_df = factor_scores_df.copy()\n",
    "regression_df[target_variable] = df[target_variable].map({'YES': 1, 'NO': 0, 'UNKNOWN': None})\n",
    "\n",
    "# Drop rows with unknown values for the target variable\n",
    "regression_df.dropna(subset=[target_variable], inplace=True)\n",
    "X = regression_df.drop(columns=[target_variable])\n",
    "y = regression_df[target_variable]\n",
    "\n",
    "# 12. Train and fit a Logistic Regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 13. Interpret the regression coefficients and save to CSV.\n",
    "print(\"\\n--- Regression Coefficients of Factors Predicting Mental Illness ---\")\n",
    "regression_coefficients = pd.DataFrame(model.coef_.T, index=X.columns, columns=['Coefficient'])\n",
    "regression_coefficients['Absolute Value'] = regression_coefficients['Coefficient'].abs()\n",
    "regression_coefficients = regression_coefficients.sort_values(by='Absolute Value', ascending=False)\n",
    "print(regression_coefficients)\n",
    "\n",
    "# Save the regression coefficients to a CSV file\n",
    "regression_coefficients.to_csv(\"regression_coefficients.csv\")\n",
    "print(\"\\nRegression coefficients saved to 'regression_coefficients.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcab372-17e5-4349-a48a-8f562476ad25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data from the CSV files created in the previous step\n",
    "try:\n",
    "    loadings_df = pd.read_csv(\"factor_loadings.csv\", index_col=0)\n",
    "    regression_coefficients = pd.read_csv(\"regression_coefficients.csv\", index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The CSV files were not found. Please run the previous analysis code first.\")\n",
    "    exit()\n",
    "\n",
    "# --- Plot 1: Factor Loadings Heatmap ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(loadings_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Factor Loadings Heatmap\", fontsize=16)\n",
    "plt.xlabel(\"Factors\", fontsize=12)\n",
    "plt.ylabel(\"Original Variables\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"factor_loadings_heatmap.png\")\n",
    "plt.show()\n",
    "print(\"Saved 'factor_loadings_heatmap.png'\")\n",
    "\n",
    "# --- Plot 2: Regression Coefficients Bar Chart ---\n",
    "# Sort the coefficients by their absolute value to make the most impactful factors easy to see\n",
    "regression_coefficients = regression_coefficients.sort_values(by='Absolute Value', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(\n",
    "    x='Coefficient',\n",
    "    y=regression_coefficients.index,\n",
    "    data=regression_coefficients,\n",
    "    orient='h',\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Impact of Factors on Mental Illness\", fontsize=16)\n",
    "plt.xlabel(\"Coefficient Value\", fontsize=12)\n",
    "plt.ylabel(\"Factors\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"regression_coefficients_bar_chart.png\")\n",
    "plt.show()\n",
    "print(\"Saved 'regression_coefficients_bar_chart.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cd8b0-f80e-4d5c-93a3-5dbe6d43237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c67ff-fdd8-4d96-a683-edf3672be6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "# Columns from the image you provided.\n",
    "columns_for_analysis = [\n",
    "    'Patient ID', 'Survey Year', 'Program Category', 'Region Served', 'Age Group',\n",
    "    'Sex', 'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Discharge Disposition',\n",
    "    'Principal Diagnostic Class'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected.drop('Patient ID', axis=1), dummy_na=False)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 5. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnostic Class'\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in df_selected[target_variable].unique() if pd.notna(cls)])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in df_selected[target_variable].unique() if pd.notna(cls)]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 6. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Model Training and Evaluation ---\n",
    "\n",
    "print(\"\\n--- Training K-Nearest Neighbors Model ---\")\n",
    "# 7. Scale the data for KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 8. Train the KNN classifier.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 9. Make predictions.\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# 10. Evaluate the model.\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# You can use the `knn_model` to find the 'k' closest patients to any new patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00946a0-5d0a-4690-ade5-b6101f44d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "!pip install pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "# This path is for the 'Female complete Data CSV.csv' file you specified.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "# Columns from the image you provided.\n",
    "columns_for_analysis = [\n",
    "    'Patient ID', 'Survey Year', 'Program Category', 'Region Served', 'Age Group',\n",
    "    'Sex', 'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Discharge Disposition',\n",
    "    'Principal Diagnostic Class'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected.drop('Patient ID', axis=1), dummy_na=False)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 5. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnostic Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "# Filter out non-string values if any\n",
    "y_labels = [cls for cls in y_labels if isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 6. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Model Training and Evaluation ---\n",
    "\n",
    "print(\"\\n--- Training K-Nearest Neighbors Model ---\")\n",
    "# 7. Scale the data for KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 8. Train the KNN classifier.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 9. Make predictions.\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# 10. Evaluate the model.\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55ef2d-c52f-47fc-9888-c02c5807d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female complete Data CSV.csv\"\n",
    "\n",
    "# This code will print all your column names.\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f385e68-9144-4492-9c8d-e591673bb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female complete Data CSV.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42387c88-cdcd-4b7f-a447-e32077164eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "# Columns from the image you provided.\n",
    "columns_for_analysis = [\n",
    "    'Patient ID', 'Survey Year', 'Program Category', 'Region Served', 'Age Group',\n",
    "    'Sex', 'Transgender', 'Sexual Orientation', 'Hispanic Ethnicity', 'Race',\n",
    "    'Living Situation', 'Household Composition', 'Preferred Language',\n",
    "    'Religious Preference', 'Veteran Status', 'Employment Status',\n",
    "    'Number Of Hours Worked Each Week', 'Education Status',\n",
    "    'Special Education Services', 'Discharge Disposition',\n",
    "    'Principal Diagnostic Class'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected.drop('Patient ID', axis=1), dummy_na=False)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 5. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnostic Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "# Filter out non-string values if any\n",
    "y_labels = [cls for cls in y_labels if isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 6. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Model Training and Evaluation ---\n",
    "\n",
    "print(\"\\n--- Training K-Nearest Neighbors Model ---\")\n",
    "# 7. Scale the data for KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 8. Train the KNN classifier.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 9. Make predictions.\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# 10. Evaluate the model.\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc89c0-4cc4-4444-a303-1ac7adc124ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure pandas is installed in this notebook session.\n",
    "!pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Your file path\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "# Check if the file exists and print column names\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Success: File found!\")\n",
    "    print(\"\\nHere are the columns in your file:\")\n",
    "    print(df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at the path: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620866f8-a1a3-43c3-af59-ab01ca139024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "!pip install scikit-learn pandas\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the complete list of columns from your file.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Sexual Orientation', 'Hispanic Ethnicity', 'Race', 'Living Situation',\n",
    "    'Household Composition', 'Preferred Language', 'Religious Preference',\n",
    "    'Veteran Status', 'Employment Status', 'Number Of Hours Worked Each Week',\n",
    "    'Education Status', 'Special Education Services', 'Mental Illness',\n",
    "    'Intellectual Disability', 'Autism Spectrum', 'Other Developmental Disability',\n",
    "    'Alcohol Related Disorder', 'Drug Substance Disorder', 'Opioid Related Disorder',\n",
    "    'Mobility Impairment Disorder', 'Hearing Impairment', 'Visual Impairment',\n",
    "    'Speech Impairment', 'Hyperlipidemia', 'High Blood Pressure', 'Diabetes',\n",
    "    'Obesity', 'Heart Attack', 'Stroke', 'Other Cardiac', 'Pulmonary Asthma',\n",
    "    'Alzheimer or Dementia', 'Kidney Disease', 'Liver Disease',\n",
    "    'Endocrine Condition', 'Neurological Condition', 'Traumatic Brain Injury',\n",
    "    'Joint Disease', 'Cancer', 'Other Chronic Med Condition',\n",
    "    'No Chronic Med Condition', 'Unknown Chronic Med Condition',\n",
    "    'Cannabis Recreational Use', 'Cannabis Medicinal Use', 'Smokes',\n",
    "    'Received Smoking Medication', 'Received Smoking Counseling',\n",
    "    'Serious Mental Illness', 'Alcohol 12m Service', 'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service', 'Principal Diagnosis Class'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "\n",
    "# 4. Handle potential missing values.\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 5. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 6. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Model Training and Evaluation ---\n",
    "\n",
    "print(\"\\n--- Training K-Nearest Neighbors Model ---\")\n",
    "# 7. Scale the data for KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 8. Train the KNN classifier.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 9. Make predictions.\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# 10. Evaluate the model.\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a939f-1e6d-4bdc-9811-8e91e957a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2816b-1132-40ab-a807-e3d7a242a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Hyperparameter Tuning ---\n",
    "print(\"\\n--- Hyperparameter Tuning: Finding the best n_neighbors ---\")\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 15, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_k = knn_model.predict(X_test)\n",
    "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
    "    print(f\"Accuracy for n_neighbors = {k}: {accuracy_k:.4f}\")\n",
    "    if accuracy_k > best_accuracy:\n",
    "        best_accuracy = accuracy_k\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nOptimal n_neighbors found: {best_k} with accuracy of {best_accuracy:.4f}\")\n",
    "\n",
    "# --- Part 3: Data Balancing with SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "\n",
    "# 1. Scale the data. Scaling is crucial before applying SMOTE.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Apply SMOTE to the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 4: Rerunning the Model with Optimized Parameters ---\n",
    "print(\"\\n--- Model Evaluation on Resampled Data with Optimal 'k' ---\")\n",
    "\n",
    "# 1. Train the KNN classifier with the best 'k' and resampled data.\n",
    "knn_model_balanced = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_model_balanced.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. Make predictions on the original, un-resampled test data.\n",
    "y_pred_balanced = knn_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model.\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "print(f\"Accuracy on original test data: {accuracy_balanced:.4f}\\n\")\n",
    "\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "print(\"Classification Report on Resampled Data:\\n\", report_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3465b2-2c74-4064-b52b-204dbdc31bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, uninstall the existing libraries\n",
    "pip uninstall imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba673a0-4a20-4769-add6-d2af31783cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Hyperparameter Tuning ---\n",
    "print(\"\\n--- Hyperparameter Tuning: Finding the best n_neighbors ---\")\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 15, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_k = knn_model.predict(X_test)\n",
    "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
    "    print(f\"Accuracy for n_neighbors = {k}: {accuracy_k:.4f}\")\n",
    "    if accuracy_k > best_accuracy:\n",
    "        best_accuracy = accuracy_k\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nOptimal n_neighbors found: {best_k} with accuracy of {best_accuracy:.4f}\")\n",
    "\n",
    "# --- Part 3: Data Balancing with SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "\n",
    "# 1. Scale the data. Scaling is crucial before applying SMOTE.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Apply SMOTE to the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 4: Rerunning the Model with Optimized Parameters ---\n",
    "print(\"\\n--- Model Evaluation on Resampled Data with Optimal 'k' ---\")\n",
    "\n",
    "# 1. Train the KNN classifier with the best 'k' and resampled data.\n",
    "knn_model_balanced = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_model_balanced.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. Make predictions on the original, un-resampled test data.\n",
    "y_pred_balanced = knn_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model.\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "print(f\"Accuracy on original test data: {accuracy_balanced:.4f}\\n\")\n",
    "\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "print(\"Classification Report on Resampled Data:\\n\", report_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2880ba-64e3-41f5-9f18-2e33c3e897bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, uninstall your current version of imblearn\n",
    "pip uninstall imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41c95a-fda1-4bf1-b852-594a5c562d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure all libraries are installed in this notebook session.\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "df = pd.read_csv(file_path)\n",
    "df_selected = df[columns_for_analysis]\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Hyperparameter Tuning ---\n",
    "print(\"\\n--- Hyperparameter Tuning: Finding the best n_neighbors ---\")\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 15, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_k = knn_model.predict(X_test)\n",
    "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
    "    print(f\"Accuracy for n_neighbors = {k}: {accuracy_k:.4f}\")\n",
    "    if accuracy_k > best_accuracy:\n",
    "        best_accuracy = accuracy_k\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nOptimal n_neighbors found: {best_k} with accuracy of {best_accuracy:.4f}\")\n",
    "\n",
    "# --- Part 3: Data Balancing with SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "\n",
    "# 1. Scale the data. Scaling is crucial before applying SMOTE.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Apply SMOTE to the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 4: Rerunning the Model with Optimized Parameters ---\n",
    "print(\"\\n--- Model Evaluation on Resampled Data with Optimal 'k' ---\")\n",
    "\n",
    "# 1. Train the KNN classifier with the best 'k' and resampled data.\n",
    "knn_model_balanced = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_model_balanced.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. Make predictions on the original, un-resampled test data.\n",
    "y_pred_balanced = knn_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model.\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "print(f\"Accuracy on original test data: {accuracy_balanced:.4f}\\n\")\n",
    "\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "print(\"Classification Report on Resampled Data:\\n\", report_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6530e42-6847-458e-988f-5db9f11cd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, uninstall the existing imblearn library\n",
    "pip uninstall imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993e8a6-888b-4059-838f-750790f2da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a41c9-fde0-43e7-86e1-879a64b922ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn==0.10.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cebac6-0137-40f4-8088-f31f197f82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8e526-62f7-4fca-8e46-ee4838b672a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn==0.10.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d4533-88cf-4e20-8803-3209af5cd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "df = pd.read_csv(file_path)\n",
    "df_selected = df[columns_for_analysis]\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Hyperparameter Tuning ---\n",
    "print(\"\\n--- Hyperparameter Tuning: Finding the best n_neighbors ---\")\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 15, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_k = knn_model.predict(X_test)\n",
    "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
    "    print(f\"Accuracy for n_neighbors = {k}: {accuracy_k:.4f}\")\n",
    "    if accuracy_k > best_accuracy:\n",
    "        best_accuracy = accuracy_k\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nOptimal n_neighbors found: {best_k} with accuracy of {best_accuracy:.4f}\")\n",
    "\n",
    "# --- Part 3: Data Balancing with SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "\n",
    "# 1. Scale the data. Scaling is crucial before applying SMOTE.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Apply SMOTE to the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 4: Rerunning the Model with Optimized Parameters ---\n",
    "print(\"\\n--- Model Evaluation on Resampled Data with Optimal 'k' ---\")\n",
    "\n",
    "# 1. Train the KNN classifier with the best 'k' and resampled data.\n",
    "knn_model_balanced = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_model_balanced.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. Make predictions on the original, un-resampled test data.\n",
    "y_pred_balanced = knn_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model.\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "print(f\"Accuracy on original test data: {accuracy_balanced:.4f}\\n\")\n",
    "\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "print(\"Classification Report on Resampled Data:\\n\", report_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c37d2-f5c7-430b-88e6-9a709deee4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "df = pd.read_csv(file_path)\n",
    "df_selected = df[columns_for_analysis]\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Data Balancing with SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "\n",
    "# 1. Scale the data. Scaling is crucial before applying SMOTE.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Apply SMOTE to the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 3: Rerunning the Model with Optimized Parameters ---\n",
    "print(\"\\n--- Model Evaluation on Resampled Data with Optimal 'k' ---\")\n",
    "\n",
    "# 1. Train the KNN classifier with the best 'k' and resampled data.\n",
    "knn_model_balanced = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model_balanced.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. Make predictions on the original, un-resampled test data.\n",
    "y_pred_balanced = knn_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model.\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "print(f\"Accuracy on original test data: {accuracy_balanced:.4f}\\n\")\n",
    "\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "print(\"Classification Report on Resampled Data:\\n\", report_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363fe90-61fd-4964-a871-a51f0163584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define file path and columns\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load a subset of the dataset\n",
    "print(\"\\n--- Loading Data ---\")\n",
    "df = pd.read_csv(file_path, usecols=columns_for_analysis, nrows=2000)  # Limit rows for speed\n",
    "\n",
    "# 3. One-hot encode and fill missing values\n",
    "print(\"--- Preprocessing Data ---\")\n",
    "df_processed = pd.get_dummies(df, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features and target\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df[target_variable].dropna().unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Scaling and PCA ---\n",
    "print(\"\\n--- Scaling and Dimensionality Reduction ---\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=30)  # You can adjust this number\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Optional: Check how much variance is retained\n",
    "print(f\"Total variance retained by PCA: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "# --- Part 3: SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "start = time.time()\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_pca, y_train)\n",
    "print(f\"SMOTE completed in {time.time() - start:.2f} seconds\")\n",
    "print(f\"Original training size: {X_train.shape[0]}, Resampled size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 4: Model Training and Evaluation ---\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = knn_model.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test data: {accuracy:.4f}\\n\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb5110-84f5-469f-94cc-eab3b33562c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Female complete Data CSV.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2df81a5-15d4-4ca2-a89c-d51d79d2f5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imbalanced-learn==0.10.1\n",
      "  Downloading imbalanced_learn-0.10.1-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (3.6.0)\n",
      "Downloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.10.1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\arunc\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     11\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\arunc\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "# First, ensure all libraries are installed and compatible in this notebook session.\n",
    "!pip install imbalanced-learn==0.10.1\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Hyperparameter Tuning ---\n",
    "print(\"\\n--- Hyperparameter Tuning: Finding the best n_neighbors ---\")\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 15, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_k = knn_model.predict(X_test)\n",
    "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
    "    print(f\"Accuracy for n_neighbors = {k}: {accuracy_k:.4f}\")\n",
    "    if accuracy_k > best_accuracy:\n",
    "        best_accuracy = accuracy_k\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nOptimal n_neighbors found: {best_k} with accuracy of {best_accuracy:.4f}\")\n",
    "\n",
    "# --- Part 3: Data Balancing with SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "\n",
    "# 1. Scale the data. Scaling is crucial before applying SMOTE.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Apply SMOTE to the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 4: Rerunning the Model with Optimized Parameters ---\n",
    "print(\"\\n--- Model Evaluation on Resampled Data with Optimal 'k' ---\")\n",
    "\n",
    "# 1. Train the KNN classifier with the best 'k' and resampled data.\n",
    "knn_model_balanced = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_model_balanced.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. Make predictions on the original, un-resampled test data.\n",
    "y_pred_balanced = knn_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model.\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "print(f\"Accuracy on original test data: {accuracy_balanced:.4f}\\n\")\n",
    "\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "print(\"Classification Report on Resampled Data:\\n\", report_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4987260-432b-41a5-a4a4-91204cf4e440",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (912299130.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip uninstall scikit-learn\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# First, uninstall the existing scikit-learn\n",
    "pip uninstall scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca658e00-6e25-4ef9-914f-4724cdc83c70",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2709331464.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install scikit-learn==1.2.2\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Then, install a compatible version\n",
    "pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0b3be7-08cf-48d8-9bd5-1778d032290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: imbalanced-learn 0.10.1\n",
      "Uninstalling imbalanced-learn-0.10.1:\n",
      "  Successfully uninstalled imbalanced-learn-0.10.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn.utils._param_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     14\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     27\u001b[0m sklearn_version \u001b[38;5;241m=\u001b[39m parse_version(sklearn\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn.utils._param_validation'"
     ]
    }
   ],
   "source": [
    "# First, uninstall the incompatible version of imblearn.\n",
    "!pip uninstall imbalanced-learn -y\n",
    "\n",
    "# Then, install the latest, compatible version.\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Part 1: Data Preparation ---\n",
    "\n",
    "# 1. Define the file path and the specific columns for analysis.\n",
    "file_path = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "\n",
    "columns_for_analysis = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "\n",
    "# 2. Load the dataset and select the specified columns.\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_selected = df[columns_for_analysis]\n",
    "    print(\"Columns loaded successfully.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A column name was not found. Please check your spelling.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check your file path.\")\n",
    "    raise\n",
    "\n",
    "# 3. Handle non-numeric data using one-hot encoding.\n",
    "df_processed = pd.get_dummies(df_selected, dummy_na=False)\n",
    "df_processed = df_processed.fillna(df_processed.mean(numeric_only=True))\n",
    "\n",
    "# 4. Separate features (X) and target (y).\n",
    "target_variable = 'Principal Diagnosis Class'\n",
    "y_labels = df_selected[target_variable].unique().tolist()\n",
    "y_labels = [cls for cls in y_labels if pd.notna(cls) and isinstance(cls, str)]\n",
    "\n",
    "X = df_processed.drop(columns=[f'{target_variable}_{cls}' for cls in y_labels])\n",
    "y = df_processed[[f'{target_variable}_{cls}' for cls in y_labels]]\n",
    "y = y.idxmax(axis=1).str.replace(f'{target_variable}_', '', regex=False)\n",
    "\n",
    "# 5. Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Part 2: Hyperparameter Tuning ---\n",
    "print(\"\\n--- Hyperparameter Tuning: Finding the best n_neighbors ---\")\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(1, 15, 2):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_k = knn_model.predict(X_test)\n",
    "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
    "    print(f\"Accuracy for n_neighbors = {k}: {accuracy_k:.4f}\")\n",
    "    if accuracy_k > best_accuracy:\n",
    "        best_accuracy = accuracy_k\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nOptimal n_neighbors found: {best_k} with accuracy of {best_accuracy:.4f}\")\n",
    "\n",
    "# --- Part 3: Data Balancing with SMOTE ---\n",
    "print(\"\\n--- Balancing the Dataset with SMOTE ---\")\n",
    "\n",
    "# 1. Scale the data. Scaling is crucial before applying SMOTE.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Apply SMOTE to the training data.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_resampled.shape[0]}\")\n",
    "\n",
    "# --- Part 4: Rerunning the Model with Optimized Parameters ---\n",
    "print(\"\\n--- Model Evaluation on Resampled Data with Optimal 'k' ---\")\n",
    "\n",
    "# 1. Train the KNN classifier with the best 'k' and resampled data.\n",
    "knn_model_balanced = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_model_balanced.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 2. Make predictions on the original, un-resampled test data.\n",
    "y_pred_balanced = knn_model_balanced.predict(X_test_scaled)\n",
    "\n",
    "# 3. Evaluate the model.\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "print(f\"Accuracy on original test data: {accuracy_balanced:.4f}\\n\")\n",
    "\n",
    "report_balanced = classification_report(y_test, y_pred_balanced)\n",
    "print(\"Classification Report on Resampled Data:\\n\", report_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67053562-616d-4600-84e2-c586822524cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1478831797.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install --upgrade imbalanced-learn scikit-learn pandas\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# First, try to upgrade all relevant libraries to fix the path conflict.\n",
    "pip install --upgrade imbalanced-learn scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da56c8c9-3025-4b8f-ac59-37fbe5ba590d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', '--upgrade', 'imbalanced-learn', 'scikit-learn', 'pandas'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Upgrade libraries via pip from within Python\n",
    "subprocess.run([\n",
    "    \"pip\", \"install\", \"--upgrade\",\n",
    "    \"imbalanced-learn\",\n",
    "    \"scikit-learn\",\n",
    "    \"pandas\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc156435-bfc3-4c26-b88b-99f17e914d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imbalanced-learn==0.10.1\n",
      "  Using cached imbalanced_learn-0.10.1-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn==0.10.1) (3.6.0)\n",
      "Using cached imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.13.0\n",
      "    Uninstalling imbalanced-learn-0.13.0:\n",
      "      Successfully uninstalled imbalanced-learn-0.13.0\n",
      "Successfully installed imbalanced-learn-0.10.1\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\arunc\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\arunc\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "# Install required version of imbalanced-learn\n",
    "!pip install imbalanced-learn==0.10.1\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_PATH = r\"C:\\Users\\arunc\\OneDrive\\Desktop\\Python Project\\Female data consolidated\\Female complete Data CSV.csv\"\n",
    "COLUMNS = [\n",
    "    'Principal Diagnosis Class',\n",
    "    'Alcohol Related Disorder',\n",
    "    'Drug Substance Disorder',\n",
    "    'Opioid Related Disorder',\n",
    "    'Serious Mental Illness',\n",
    "    'Alcohol 12m Service',\n",
    "    'Opioid 12m Service',\n",
    "    'Drug/Substance 12m Service'\n",
    "]\n",
    "TARGET = 'Principal Diagnosis Class'\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Load & Prepare Data ---\n",
    "def load_data(path, columns):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df_selected = df[columns]\n",
    "        print(\"Columns loaded successfully.\")\n",
    "        return df_selected\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Column name error: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"File not found. Check your path.\")\n",
    "\n",
    "def preprocess_data(df, target):\n",
    "    df_encoded = pd.get_dummies(df, dummy_na=False)\n",
    "    df_encoded.fillna(df_encoded.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    y_classes = [cls for cls in df[target].unique() if pd.notna(cls) and isinstance(cls, str)]\n",
    "    y = df_encoded[[f'{target}_{cls}' for cls in y_classes]].idxmax(axis=1).str.replace(f'{target}_', '', regex=False)\n",
    "    X = df_encoded.drop(columns=[f'{target}_{cls}' for cls in y_classes])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# --- Model Tuning ---\n",
    "def find_best_k(X_train, y_train, X_test, y_test, k_range=range(1, 15, 2)):\n",
    "    print(\"\\nTuning hyperparameter 'n_neighbors'...\")\n",
    "    best_k, best_acc = 0, 0\n",
    "    for k in k_range:\n",
    "        model = KNeighborsClassifier(n_neighbors=k)\n",
    "        model.fit(X_train, y_train)\n",
    "        acc = accuracy_score(y_test, model.predict(X_test))\n",
    "        print(f\"n_neighbors = {k}: Accuracy = {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_k, best_acc = k, acc\n",
    "    print(f\"\\nBest n_neighbors: {best_k} with accuracy: {best_acc:.4f}\")\n",
    "    return best_k\n",
    "\n",
    "# --- Resampling & Evaluation ---\n",
    "def balance_and_evaluate(X_train, y_train, X_test, y_test, k):\n",
    "    print(\"\\nApplying SMOTE and evaluating model...\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    print(f\"Original training size: {len(X_train)}\")\n",
    "    print(f\"Resampled training size: {len(X_resampled)}\")\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nAccuracy on test data: {acc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Run Pipeline ---\n",
    "df_raw = load_data(FILE_PATH, COLUMNS)\n",
    "X, y = preprocess_data(df_raw, TARGET)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "best_k = find_best_k(X_train, y_train, X_test, y_test)\n",
    "balance_and_evaluate(X_train, y_train, X_test, y_test, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b304370-0f2b-4f67-8617-90fb50330e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (0.10.1)\n",
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\arunc\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.10.1\n",
      "    Uninstalling imbalanced-learn-0.10.1:\n",
      "      Successfully uninstalled imbalanced-learn-0.10.1\n",
      "Successfully installed imbalanced-learn-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3273343f-2973-48b4-a431-5294ec92b96b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (C:\\Users\\arunc\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneToOneFeatureMixin\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _OneToOneFeatureMixin \u001b[38;5;28;01mas\u001b[39;00m OneToOneFeatureMixin\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m label_binarize\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\utils\\_sklearn_compat.py:815\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_dataclass_args())\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTags\u001b[39;00m(Tags):\n\u001b[0;32m    813\u001b[0m     sampler_tags: SamplerTags \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_test_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstance_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    816\u001b[0m     _construct_instances,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    817\u001b[0m )\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator_checks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    819\u001b[0m     check_estimator,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    820\u001b[0m     parametrize_with_checks,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    821\u001b[0m )\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_of_target  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_test_common\\instance_generator.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone, config_context\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalibration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CalibratedClassifierCV\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     HDBSCAN,\n\u001b[0;32m     15\u001b[0m     AffinityPropagation,\n\u001b[0;32m     16\u001b[0m     AgglomerativeClustering,\n\u001b[0;32m     17\u001b[0m     Birch,\n\u001b[0;32m     18\u001b[0m     BisectingKMeans,\n\u001b[0;32m     19\u001b[0m     FeatureAgglomeration,\n\u001b[0;32m     20\u001b[0m     KMeans,\n\u001b[0;32m     21\u001b[0m     MeanShift,\n\u001b[0;32m     22\u001b[0m     MiniBatchKMeans,\n\u001b[0;32m     23\u001b[0m     SpectralBiclustering,\n\u001b[0;32m     24\u001b[0m     SpectralClustering,\n\u001b[0;32m     25\u001b[0m     SpectralCoclustering,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcovariance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphicalLasso, GraphicalLassoCV\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\__init__.py:7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_affinity_propagation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AffinityPropagation, affinity_propagation\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_agglomerative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     AgglomerativeClustering,\n\u001b[0;32m      9\u001b[0m     FeatureAgglomeration,\n\u001b[0;32m     10\u001b[0m     linkage_tree,\n\u001b[0;32m     11\u001b[0m     ward_tree,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpectralBiclustering, SpectralCoclustering\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_birch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Birch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_agglomerative.py:44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# mypy error: Module 'sklearn.cluster' has no attribute '_hierarchical_fast'\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _hierarchical_fast \u001b[38;5;28;01mas\u001b[39;00m _hierarchical  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_feature_agglomeration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgglomerationTransform\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# For non fully-connected graphs\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fix_connectivity\u001b[39m(X, connectivity, affinity):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_feature_agglomeration.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerMixin\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_Xt_in_inverse_transform\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_is_fitted, validate_data\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Mixin class for feature agglomeration.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (C:\\Users\\arunc\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\deprecation.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81017f-fc9d-42c8-a9ab-2c19ac263150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
